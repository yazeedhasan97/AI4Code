{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6c4d21ea-0139-4c44-af1b-6f154c738d8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\yazee\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\yazee\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import json, shutil\n",
    "import os, re, sys\n",
    "from pathlib import Path\n",
    "import glob, time\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "\n",
    "from tqdm import tqdm\n",
    "from tqdm import notebook\n",
    "notebook.tqdm.pandas()\n",
    "tqdm.pandas()\n",
    "\n",
    "import psutil\n",
    "from pyxtension.streams import stream\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams[\"figure.figsize\"] = (20, 12)\n",
    "\n",
    "\n",
    "import preprocessers as r\n",
    "import pt_model as m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1c957e3c-5f1a-4555-8c92-0b116cabb98f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[94mNumber of notebooks present in train set  =  139256\n",
      "\u001b[94mNumber of notebooks contribute in training =  100\n"
     ]
    }
   ],
   "source": [
    "LOAD_NUM = 100\n",
    "RANDOM_SEED = 42\n",
    "PROCESSORS_COUNT = psutil.cpu_count(logical=False)\n",
    "MULTI = PROCESSORS_COUNT * int(str(LOAD_NUM) [:-2])\n",
    "\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "data_dir = Path('.')\n",
    "train_dir = os.path.join(data_dir, 'train')\n",
    "\n",
    "models_dir = os.path.join(data_dir, 'pt_models')\n",
    "orders_path = os.path.join(data_dir, 'train_orders.csv')\n",
    "ancestors_path = os.path.join(data_dir, 'train_ancestors.csv')\n",
    "\n",
    "\n",
    "\n",
    "# shutil.rmtree(models_dir)\n",
    "# if not os.path.exists(models_dir):\n",
    "#     os.mkdir(models_dir)\n",
    "count = len(list(glob.iglob(os.path.join(train_dir, '*.json'))))\n",
    "# LOAD_NUM = int(count * 0.1) + 1\n",
    "print(f\"\\033[94mNumber of notebooks present in train set  = \", count)\n",
    "print(f\"\\033[94mNumber of notebooks contribute in training = \", LOAD_NUM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0a279a6f-dc9d-4d5c-8622-051151bd7fae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train NBs: 100%|██████████| 100/100 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "MultiIndex: 4427 entries, ('00001756c60be8', '1862f0a6') to ('002bcc9e2f9077', 'cffa684c')\n",
      "Data columns (total 2 columns):\n",
      " #   Column     Non-Null Count  Dtype   \n",
      "---  ------     --------------  -----   \n",
      " 0   cell_type  4427 non-null   category\n",
      " 1   source     4427 non-null   object  \n",
      "dtypes: category(1), object(1)\n",
      "memory usage: 220.6+ KB\n",
      "None\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>cell_type</th>\n",
       "      <th>source</th>\n",
       "      <th>n_code_cells</th>\n",
       "      <th>n_markdown_cells</th>\n",
       "      <th>words_count</th>\n",
       "      <th>letters_count</th>\n",
       "      <th>empty_lines_count</th>\n",
       "      <th>comment_lines_count</th>\n",
       "      <th>full_lines_count</th>\n",
       "      <th>text_lines_count</th>\n",
       "      <th>tag_lines_count</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th>cell_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">00001756c60be8</th>\n",
       "      <th>1862f0a6</th>\n",
       "      <td>code</td>\n",
       "      <td>#  This Python 3 environment comes with many h...</td>\n",
       "      <td>0.517241</td>\n",
       "      <td>0.482759</td>\n",
       "      <td>140</td>\n",
       "      <td>930</td>\n",
       "      <td>0.235294</td>\n",
       "      <td>0.411765</td>\n",
       "      <td>0.352941</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2a9e43d6</th>\n",
       "      <td>code</td>\n",
       "      <td>import numpy as np\\nimport pandas as pd\\nimpor...</td>\n",
       "      <td>0.517241</td>\n",
       "      <td>0.482759</td>\n",
       "      <td>55</td>\n",
       "      <td>498</td>\n",
       "      <td>0.176471</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.823529</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>038b763d</th>\n",
       "      <td>code</td>\n",
       "      <td>import warnings\\nwarnings.filterwarnings('igno...</td>\n",
       "      <td>0.517241</td>\n",
       "      <td>0.482759</td>\n",
       "      <td>3</td>\n",
       "      <td>49</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2eefe0ef</th>\n",
       "      <td>code</td>\n",
       "      <td>matplotlib.rcParams.update({'font.size': 14})</td>\n",
       "      <td>0.517241</td>\n",
       "      <td>0.482759</td>\n",
       "      <td>2</td>\n",
       "      <td>45</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0beab1cd</th>\n",
       "      <td>code</td>\n",
       "      <td>def evaluate_preds(train_true_values, train_pr...</td>\n",
       "      <td>0.517241</td>\n",
       "      <td>0.482759</td>\n",
       "      <td>39</td>\n",
       "      <td>694</td>\n",
       "      <td>0.052632</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.947368</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        cell_type  \\\n",
       "id             cell_id              \n",
       "00001756c60be8 1862f0a6      code   \n",
       "               2a9e43d6      code   \n",
       "               038b763d      code   \n",
       "               2eefe0ef      code   \n",
       "               0beab1cd      code   \n",
       "\n",
       "                                                                    source  \\\n",
       "id             cell_id                                                       \n",
       "00001756c60be8 1862f0a6  #  This Python 3 environment comes with many h...   \n",
       "               2a9e43d6  import numpy as np\\nimport pandas as pd\\nimpor...   \n",
       "               038b763d  import warnings\\nwarnings.filterwarnings('igno...   \n",
       "               2eefe0ef      matplotlib.rcParams.update({'font.size': 14})   \n",
       "               0beab1cd  def evaluate_preds(train_true_values, train_pr...   \n",
       "\n",
       "                         n_code_cells  n_markdown_cells  words_count  \\\n",
       "id             cell_id                                                 \n",
       "00001756c60be8 1862f0a6      0.517241          0.482759          140   \n",
       "               2a9e43d6      0.517241          0.482759           55   \n",
       "               038b763d      0.517241          0.482759            3   \n",
       "               2eefe0ef      0.517241          0.482759            2   \n",
       "               0beab1cd      0.517241          0.482759           39   \n",
       "\n",
       "                         letters_count  empty_lines_count  \\\n",
       "id             cell_id                                      \n",
       "00001756c60be8 1862f0a6            930           0.235294   \n",
       "               2a9e43d6            498           0.176471   \n",
       "               038b763d             49           0.000000   \n",
       "               2eefe0ef             45           0.000000   \n",
       "               0beab1cd            694           0.052632   \n",
       "\n",
       "                         comment_lines_count  full_lines_count  \\\n",
       "id             cell_id                                           \n",
       "00001756c60be8 1862f0a6             0.411765          0.352941   \n",
       "               2a9e43d6             0.000000          0.823529   \n",
       "               038b763d             0.000000          1.000000   \n",
       "               2eefe0ef             0.000000          1.000000   \n",
       "               0beab1cd             0.000000          0.947368   \n",
       "\n",
       "                         text_lines_count  tag_lines_count  \n",
       "id             cell_id                                      \n",
       "00001756c60be8 1862f0a6               0.0              0.0  \n",
       "               2a9e43d6               0.0              0.0  \n",
       "               038b763d               0.0              0.0  \n",
       "               2eefe0ef               0.0              0.0  \n",
       "               0beab1cd               0.0              0.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:00<00:00, 29922.98it/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "392868e7cb3e4823916bdf3dc7ea9763",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pandas Apply:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>cell_id</th>\n",
       "      <th>cell_type</th>\n",
       "      <th>source</th>\n",
       "      <th>n_code_cells</th>\n",
       "      <th>n_markdown_cells</th>\n",
       "      <th>words_count</th>\n",
       "      <th>letters_count</th>\n",
       "      <th>empty_lines_count</th>\n",
       "      <th>comment_lines_count</th>\n",
       "      <th>full_lines_count</th>\n",
       "      <th>text_lines_count</th>\n",
       "      <th>tag_lines_count</th>\n",
       "      <th>rank</th>\n",
       "      <th>ancestor_id</th>\n",
       "      <th>parent_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00001756c60be8</td>\n",
       "      <td>1862f0a6</td>\n",
       "      <td>code</td>\n",
       "      <td>#  This Python 3 environment comes with many h...</td>\n",
       "      <td>0.517241</td>\n",
       "      <td>0.482759</td>\n",
       "      <td>140</td>\n",
       "      <td>930</td>\n",
       "      <td>0.235294</td>\n",
       "      <td>0.411765</td>\n",
       "      <td>0.352941</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>945aea18</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>00001756c60be8</td>\n",
       "      <td>2a9e43d6</td>\n",
       "      <td>code</td>\n",
       "      <td>import numpy as np\\nimport pandas as pd\\nimpor...</td>\n",
       "      <td>0.517241</td>\n",
       "      <td>0.482759</td>\n",
       "      <td>55</td>\n",
       "      <td>498</td>\n",
       "      <td>0.176471</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.823529</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.034483</td>\n",
       "      <td>945aea18</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00001756c60be8</td>\n",
       "      <td>038b763d</td>\n",
       "      <td>code</td>\n",
       "      <td>import warnings\\nwarnings.filterwarnings('igno...</td>\n",
       "      <td>0.517241</td>\n",
       "      <td>0.482759</td>\n",
       "      <td>3</td>\n",
       "      <td>49</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.068966</td>\n",
       "      <td>945aea18</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00001756c60be8</td>\n",
       "      <td>2eefe0ef</td>\n",
       "      <td>code</td>\n",
       "      <td>matplotlib.rcParams.update({'font.size': 14})</td>\n",
       "      <td>0.517241</td>\n",
       "      <td>0.482759</td>\n",
       "      <td>2</td>\n",
       "      <td>45</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.103448</td>\n",
       "      <td>945aea18</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00001756c60be8</td>\n",
       "      <td>0beab1cd</td>\n",
       "      <td>code</td>\n",
       "      <td>def evaluate_preds(train_true_values, train_pr...</td>\n",
       "      <td>0.517241</td>\n",
       "      <td>0.482759</td>\n",
       "      <td>39</td>\n",
       "      <td>694</td>\n",
       "      <td>0.052632</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.947368</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.137931</td>\n",
       "      <td>945aea18</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               id   cell_id cell_type  \\\n",
       "0  00001756c60be8  1862f0a6      code   \n",
       "1  00001756c60be8  2a9e43d6      code   \n",
       "2  00001756c60be8  038b763d      code   \n",
       "3  00001756c60be8  2eefe0ef      code   \n",
       "4  00001756c60be8  0beab1cd      code   \n",
       "\n",
       "                                              source  n_code_cells  \\\n",
       "0  #  This Python 3 environment comes with many h...      0.517241   \n",
       "1  import numpy as np\\nimport pandas as pd\\nimpor...      0.517241   \n",
       "2  import warnings\\nwarnings.filterwarnings('igno...      0.517241   \n",
       "3      matplotlib.rcParams.update({'font.size': 14})      0.517241   \n",
       "4  def evaluate_preds(train_true_values, train_pr...      0.517241   \n",
       "\n",
       "   n_markdown_cells  words_count  letters_count  empty_lines_count  \\\n",
       "0          0.482759          140            930           0.235294   \n",
       "1          0.482759           55            498           0.176471   \n",
       "2          0.482759            3             49           0.000000   \n",
       "3          0.482759            2             45           0.000000   \n",
       "4          0.482759           39            694           0.052632   \n",
       "\n",
       "   comment_lines_count  full_lines_count  text_lines_count  tag_lines_count  \\\n",
       "0             0.411765          0.352941               0.0              0.0   \n",
       "1             0.000000          0.823529               0.0              0.0   \n",
       "2             0.000000          1.000000               0.0              0.0   \n",
       "3             0.000000          1.000000               0.0              0.0   \n",
       "4             0.000000          0.947368               0.0              0.0   \n",
       "\n",
       "       rank ancestor_id parent_id  \n",
       "0  0.000000    945aea18       NaN  \n",
       "1  0.034483    945aea18       NaN  \n",
       "2  0.068966    945aea18       NaN  \n",
       "3  0.103448    945aea18       NaN  \n",
       "4  0.137931    945aea18       NaN  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duration 11.577303171157837 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "df = r.read_all_notebooks_(train_dir, LOAD_NUM, PROCESSORS_COUNT)\n",
    "\n",
    "df = pd.concat(stream(np.array_split(df, PROCESSORS_COUNT)).mpmap(r.extract_features))\n",
    "\n",
    "display(df.head())\n",
    "\n",
    "print('-' * 125)\n",
    "\n",
    "# Read Ordering data\n",
    "df_orders = pd.read_csv(\n",
    "    orders_path,\n",
    "    index_col='id',\n",
    ")\n",
    "df_orders['cell_order'] = df_orders['cell_order'].str.split()  # Split the string representation of cell_ids into a list\n",
    "df_orders = df_orders.squeeze(axis=1)\n",
    "\n",
    "\n",
    "# build ranks as integers \n",
    "df = df.join(r.build_ranks_(df_orders, df, PROCESSORS_COUNT))\n",
    "\n",
    "\n",
    "# Read Ancestors data\n",
    "df = df.reset_index().merge(pd.read_csv(ancestors_path,  index_col='id'), on=[\"id\"])\n",
    "\n",
    "# convert integer ranks to percentages \n",
    "df[\"rank\"] = df[\"rank\"] / df.groupby(\"id\")[\"cell_id\"].transform(\"count\")\n",
    "\n",
    "display(df.head())\n",
    "\n",
    "rduration = time.time() - start_time\n",
    "print(f\"Duration {rduration} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7d3eb2ab-052d-4dd0-9ac4-03da4c7e9f4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8b1af4a5-f509-4de8-8763-fd6bc391a3a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for percentages \n",
    "INTREST_PERCENT = 0.95\n",
    "CODE_TYPE = 'code'\n",
    "MKDN_TYPE = 'markdown'\n",
    "VALIDATION_RATIO = 0.15\n",
    "\n",
    "NOT_GENERATED_COLUMNS = ['id', 'cell_id', 'source', 'cell_type', 'rank', 'ancestor_id', 'parent_id', ]\n",
    "MODEL_USELESS = ['id', 'cell_id', 'cell_type', 'ancestor_id', 'parent_id', ]\n",
    "GENERATED_COLUMNS_COUNT = len(df.drop(['id', 'cell_id', 'source', 'cell_type', 'rank', 'ancestor_id', 'parent_id', ], axis=1).columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d0dff9dd-f411-4717-b440-630ba647d5c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5dfdfba0-6e8e-4195-89b7-50787ffb9bc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__CUDA VERSION: 11.5\n",
      "__CUDNN VERSION: 8302\n",
      "True\n",
      "None\n",
      "|===========================================================================|\n",
      "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
      "|---------------------------------------------------------------------------|\n",
      "|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\n",
      "|===========================================================================|\n",
      "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocated memory      |       0 B  |       0 B  |       0 B  |       0 B  |\n",
      "|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |\n",
      "|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active memory         |       0 B  |       0 B  |       0 B  |       0 B  |\n",
      "|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |\n",
      "|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved memory   |       0 B  |       0 B  |       0 B  |       0 B  |\n",
      "|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |\n",
      "|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable memory |       0 B  |       0 B  |       0 B  |       0 B  |\n",
      "|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |\n",
      "|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocations           |       0    |       0    |       0    |       0    |\n",
      "|       from large pool |       0    |       0    |       0    |       0    |\n",
      "|       from small pool |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active allocs         |       0    |       0    |       0    |       0    |\n",
      "|       from large pool |       0    |       0    |       0    |       0    |\n",
      "|       from small pool |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved segments |       0    |       0    |       0    |       0    |\n",
      "|       from large pool |       0    |       0    |       0    |       0    |\n",
      "|       from small pool |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable allocs |       0    |       0    |       0    |       0    |\n",
      "|       from large pool |       0    |       0    |       0    |       0    |\n",
      "|       from small pool |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
      "|===========================================================================|\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch as t\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "\n",
    "\n",
    "from copy import deepcopy\n",
    "\n",
    "print('__CUDA VERSION:', t.version.cuda)\n",
    "print('__CUDNN VERSION:', t.backends.cudnn.version())\n",
    "print(t.cuda.is_available())\n",
    "if t.cuda.is_available():\n",
    "    print(t.cuda.empty_cache())\n",
    "    print(t.cuda.memory_summary(device=None, abbreviated=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "465fc851-7dc4-4f2b-af4d-7f88fced939e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2 3 4] [151 152 153 154 155]\n"
     ]
    }
   ],
   "source": [
    "splitter = GroupShuffleSplit(n_splits=1, test_size=VALIDATION_RATIO, random_state=RANDOM_SEED)\n",
    "\n",
    "def extract_items(ids, data, cell_type):\n",
    "    tmp = data.loc[ids, :].reset_index(drop=True)\n",
    "    return tmp[tmp.cell_type == cell_type]\n",
    "\n",
    "\n",
    "# Split, keeping notebooks with a common origin (ancestor_id) together\n",
    "ids_train, ids_valid = next(splitter.split(df, groups=df[\"ancestor_id\"]))\n",
    "print(ids_train[:5], ids_valid[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f5fe8bb4-c478-4195-ad2d-af97da41b4b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3452801a-fc2f-4270-b347-22e68b74457f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words counts to cover 0.95 is: 196\n",
      "Total number of fetures output from bert: 205\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/graphcodebert-base were not used when initializing RobertaModel: ['lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.decoder.bias', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at microsoft/graphcodebert-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 0.2625 lr: 1e-05: 100%|██████████| 299/299 [01:23<00:00,  3.60it/s]\n",
      "100%|██████████| 82/82 [00:34<00:00,  2.40it/s]\n",
      "Validation MAE: 0.1212\n",
      "\n",
      "Final accuracy for code is: 0.5713351052954406\n",
      "\n",
      "Duration 135.9096417427063 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "# BERT_MODEL_NAME = \"microsoft/codebert-base\"\n",
    "BERT_MODEL_NAME = 'distilbert-base-uncased'\n",
    "# BERT_MODEL_NAME = \"microsoft/graphcodebert-base\"\n",
    "\n",
    "# OPTIMIZER = 'adam'\n",
    "OPTIMIZER = 'nadam'\n",
    "\n",
    "\n",
    "\n",
    "# CD_MAX_LENGTH = int(df[df.cell_type == CODE_TYPE].words_count.quantile(INTREST_PERCENT)) # max length our sequences will be (e.g. how many words from a Tweet does our model see?)\n",
    "CD_MAX_LENGTH = 196\n",
    "BERT_OUTPUT_FEATURES = CD_MAX_LENGTH + GENERATED_COLUMNS_COUNT\n",
    "\n",
    "\n",
    "print(\"Words counts to cover {percent} is: {count}\".format(percent=INTREST_PERCENT, count=CD_MAX_LENGTH))\n",
    "print(f'Total number of fetures output from bert: {BERT_OUTPUT_FEATURES}')\n",
    "\n",
    "ACCUMULATION_SETPS = 3\n",
    "\n",
    "# model run\n",
    "BATCH_SIZE = 8\n",
    "EPOCHS = 1\n",
    "TOTAL_MAX_LEN = 285\n",
    "\n",
    "\n",
    "# extract code cells for each notebook\n",
    "code_df_train = extract_items(ids_train, df, CODE_TYPE)\n",
    "code_df_valid = extract_items(ids_valid, df, CODE_TYPE)\n",
    "# print(code_df_train[:5], code_df_valid[:5])\n",
    "\n",
    "# build code Dataset\n",
    "cd_train_ds = m.BDataset(\n",
    "    code_df_train, \n",
    "    max_len=CD_MAX_LENGTH, \n",
    "    bert_model_name=BERT_MODEL_NAME, \n",
    "    total_max_len=TOTAL_MAX_LEN, \n",
    "    drop=MODEL_USELESS\n",
    ")\n",
    "cd_val_ds = m.BDataset(\n",
    "    code_df_valid, \n",
    "    max_len=CD_MAX_LENGTH, \n",
    "    bert_model_name=BERT_MODEL_NAME, \n",
    "    total_max_len=TOTAL_MAX_LEN,  \n",
    "    drop=MODEL_USELESS\n",
    ")\n",
    "\n",
    "# print(cd_train_ds[0], cd_val_ds[0])\n",
    "\n",
    "# build code DataLoader\n",
    "cd_train_loader = DataLoader(cd_train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=PROCESSORS_COUNT, pin_memory=False, drop_last=True)\n",
    "cd_val_loader = DataLoader(cd_val_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=PROCESSORS_COUNT, pin_memory=False, drop_last=False)\n",
    "# print(cd_train_loader, cd_val_loader)\n",
    "\n",
    "########################################################################################################################\n",
    "code_model = m.BModel(\n",
    "    BERT_MODEL_NAME, \n",
    "    GENERATED_COLUMNS_COUNT,\n",
    ").cuda()\n",
    "\n",
    "code_model, code_y_pred = m.train(\n",
    "    code_model, \n",
    "    cd_train_loader, \n",
    "    cd_val_loader, \n",
    "    epochs=EPOCHS, \n",
    "    accumulation_steps=ACCUMULATION_SETPS, \n",
    "    model_name=BERT_MODEL_NAME,\n",
    "    opt=OPTIMIZER, \n",
    "    path=os.path.join(models_dir, 'code_bert_checkpoint.pt')\n",
    ")\n",
    "\n",
    "code_df_valid[\"pred\"] = code_df_valid.groupby([\"id\", \"cell_type\"])[\"rank\"].rank(pct=True)\n",
    "code_df_valid[\"pred\"] = code_y_pred\n",
    "\n",
    "########################################################################################################################\n",
    "code_y_dummy = code_df_valid.sort_values(\"pred\").groupby('id')['cell_id'].apply(list)\n",
    "print('Final accuracy for code is:', r.kendall_tau(df_orders.loc[code_y_dummy.index], code_y_dummy))\n",
    "\n",
    "# best_model_state = deepcopy(code_model.state_dict())\n",
    "# t.save(best_model_state, f'./pt_models/code_model_state_dict.pt')\n",
    "# t.save(code_model, f'./pt_models/code_model.pt')\n",
    "\n",
    "rduration = time.time() - start_time\n",
    "print(f\"\\nDuration {rduration} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bfba1916-9244-4a65-b3e2-8cd7e100c3f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ee91c22b-f832-4691-958c-2712870b8d0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words counts to cover 0.95 is: 128\n",
      "Total number of fetures output from bert: 137\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12fb2bacab5e47a2818550b0df1c5fad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0da611d3245f42408d855ed9a975537d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/878k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a0303c5737a4f87aaa322f5cf66dda4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/446k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "411dd1fbb275470088aeb401a499ea08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/772 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/graphcodebert-base were not used when initializing RobertaModel: ['lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.decoder.bias', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at microsoft/graphcodebert-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 0.3306 lr: 1e-05: 100%|██████████| 135/135 [00:45<00:00,  2.94it/s]\n",
      "100%|██████████| 37/37 [00:30<00:00,  1.21it/s]\n",
      "Validation MAE: 0.2637\n",
      "\n",
      "Final accuracy for markdown is: 0.8695972193825394\n",
      "\n",
      "Duration 99.59318208694458 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "# BERT_MODEL_NAME = \"microsoft/codebert-base\"\n",
    "BERT_MODEL_NAME = 'distilbert-base-uncased'\n",
    "# BERT_MODEL_NAME = \"microsoft/graphcodebert-base\"\n",
    "\n",
    "\n",
    "# OPTIMIZER = 'adam'\n",
    "OPTIMIZER = 'nadam'\n",
    "\n",
    "# MK_MAX_LENGTH = int(df[df.cell_type == MKDN_TYPE].words_count.quantile(INTREST_PERCENT)) \n",
    "MK_MAX_LENGTH = 128\n",
    "BERT_OUTPUT_FEATURES = MK_MAX_LENGTH + GENERATED_COLUMNS_COUNT\n",
    "\n",
    "\n",
    "print(\"Words counts to cover {percent} is: {count}\".format(percent=INTREST_PERCENT, count=MK_MAX_LENGTH))\n",
    "print(f'Total number of fetures output from bert: {BERT_OUTPUT_FEATURES}')\n",
    "\n",
    "ACCUMULATION_SETPS = 3\n",
    "\n",
    "# model run\n",
    "BATCH_SIZE = 8\n",
    "EPOCHS = 1\n",
    "TOTAL_MAX_LEN = 256\n",
    "\n",
    "\n",
    "# extract markdown cells for each notebook\n",
    "mkdn_df_train = extract_items(ids_train, df, MKDN_TYPE)\n",
    "mkdn_df_valid = extract_items(ids_valid, df, MKDN_TYPE)\n",
    "# print(mkdn_df_train[:5], mkdn_df_valid[:5])\n",
    "\n",
    "\n",
    "# build markdown Dataset\n",
    "mkdn_train_ds = m.BDataset(\n",
    "    mkdn_df_train, \n",
    "    max_len=MK_MAX_LENGTH, \n",
    "    bert_model_name=BERT_MODEL_NAME, \n",
    "    total_max_len=TOTAL_MAX_LEN, \n",
    "    drop=MODEL_USELESS,\n",
    ")\n",
    "mkdn_val_ds = m.BDataset(\n",
    "    mkdn_df_valid, \n",
    "    max_len=MK_MAX_LENGTH, \n",
    "    bert_model_name=BERT_MODEL_NAME, \n",
    "    total_max_len=TOTAL_MAX_LEN,  \n",
    "    drop=MODEL_USELESS,\n",
    ")\n",
    "# print(mkdn_train_ds[0], mkdn_val_ds[0])\n",
    "\n",
    "# build markdown DataLoader\n",
    "mkdn_train_loader = DataLoader(mkdn_train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=PROCESSORS_COUNT, pin_memory=False, drop_last=True)\n",
    "mkdn_val_loader = DataLoader(mkdn_val_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=PROCESSORS_COUNT, pin_memory=False, drop_last=False)\n",
    "# print(mkdn_train_loader, mkdn_val_loader)\n",
    "\n",
    "########################################################################################################################\n",
    "mkdn_model = m.BModel(\n",
    "    BERT_MODEL_NAME, \n",
    "    GENERATED_COLUMNS_COUNT,\n",
    "    # catch_path=models_dir,\n",
    ").cuda()\n",
    "\n",
    "mkdn_model, mkdn_y_pred = m.train(\n",
    "    mkdn_model, \n",
    "    mkdn_train_loader, \n",
    "    mkdn_val_loader, \n",
    "    epochs=EPOCHS, \n",
    "    accumulation_steps=ACCUMULATION_SETPS, \n",
    "    model_name=BERT_MODEL_NAME,\n",
    "    opt='nadam', \n",
    "    path=os.path.join(models_dir, 'markdown_bert_checkpoint.pt')\n",
    ")\n",
    "\n",
    "mkdn_df_valid[\"pred\"] = mkdn_df_train.groupby([\"id\", \"cell_type\"])[\"rank\"].rank(pct=True)\n",
    "mkdn_df_valid[\"pred\"] = mkdn_y_pred\n",
    "\n",
    "########################################################################################################################\n",
    "\n",
    "mkdn_y_dummy = mkdn_df_valid.sort_values(\"pred\").groupby('id')['cell_id'].apply(list)\n",
    "print('Final accuracy for markdown is:', r.kendall_tau(df_orders.loc[mkdn_y_dummy.index], mkdn_y_dummy))\n",
    "\n",
    "# # best_model_state = deepcopy(mkdn_model.state_dict())\n",
    "# # t.save(best_model_state, f'./pt_models/markdown_model_state_dict.pt')\n",
    "# # t.save(mkdn_model, f'./pt_models/markdown_model.pt')\n",
    "\n",
    "rduration = time.time() - start_time\n",
    "print(f\"\\nDuration {rduration} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "67a0715a-ae21-439c-9eed-3735e2540ebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a3aed180-0fea-44b4-911b-70b439817dfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_projector.bias', 'vocab_transform.weight', 'vocab_projector.weight', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_transform.bias']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for BModel:\n\tMissing key(s) in state_dict: \"distill_bert.transformer.layer.0.attention.q_lin.weight\", \"distill_bert.transformer.layer.0.attention.q_lin.bias\", \"distill_bert.transformer.layer.0.attention.k_lin.weight\", \"distill_bert.transformer.layer.0.attention.k_lin.bias\", \"distill_bert.transformer.layer.0.attention.v_lin.weight\", \"distill_bert.transformer.layer.0.attention.v_lin.bias\", \"distill_bert.transformer.layer.0.attention.out_lin.weight\", \"distill_bert.transformer.layer.0.attention.out_lin.bias\", \"distill_bert.transformer.layer.0.sa_layer_norm.weight\", \"distill_bert.transformer.layer.0.sa_layer_norm.bias\", \"distill_bert.transformer.layer.0.ffn.lin1.weight\", \"distill_bert.transformer.layer.0.ffn.lin1.bias\", \"distill_bert.transformer.layer.0.ffn.lin2.weight\", \"distill_bert.transformer.layer.0.ffn.lin2.bias\", \"distill_bert.transformer.layer.0.output_layer_norm.weight\", \"distill_bert.transformer.layer.0.output_layer_norm.bias\", \"distill_bert.transformer.layer.1.attention.q_lin.weight\", \"distill_bert.transformer.layer.1.attention.q_lin.bias\", \"distill_bert.transformer.layer.1.attention.k_lin.weight\", \"distill_bert.transformer.layer.1.attention.k_lin.bias\", \"distill_bert.transformer.layer.1.attention.v_lin.weight\", \"distill_bert.transformer.layer.1.attention.v_lin.bias\", \"distill_bert.transformer.layer.1.attention.out_lin.weight\", \"distill_bert.transformer.layer.1.attention.out_lin.bias\", \"distill_bert.transformer.layer.1.sa_layer_norm.weight\", \"distill_bert.transformer.layer.1.sa_layer_norm.bias\", \"distill_bert.transformer.layer.1.ffn.lin1.weight\", \"distill_bert.transformer.layer.1.ffn.lin1.bias\", \"distill_bert.transformer.layer.1.ffn.lin2.weight\", \"distill_bert.transformer.layer.1.ffn.lin2.bias\", \"distill_bert.transformer.layer.1.output_layer_norm.weight\", \"distill_bert.transformer.layer.1.output_layer_norm.bias\", \"distill_bert.transformer.layer.2.attention.q_lin.weight\", \"distill_bert.transformer.layer.2.attention.q_lin.bias\", \"distill_bert.transformer.layer.2.attention.k_lin.weight\", \"distill_bert.transformer.layer.2.attention.k_lin.bias\", \"distill_bert.transformer.layer.2.attention.v_lin.weight\", \"distill_bert.transformer.layer.2.attention.v_lin.bias\", \"distill_bert.transformer.layer.2.attention.out_lin.weight\", \"distill_bert.transformer.layer.2.attention.out_lin.bias\", \"distill_bert.transformer.layer.2.sa_layer_norm.weight\", \"distill_bert.transformer.layer.2.sa_layer_norm.bias\", \"distill_bert.transformer.layer.2.ffn.lin1.weight\", \"distill_bert.transformer.layer.2.ffn.lin1.bias\", \"distill_bert.transformer.layer.2.ffn.lin2.weight\", \"distill_bert.transformer.layer.2.ffn.lin2.bias\", \"distill_bert.transformer.layer.2.output_layer_norm.weight\", \"distill_bert.transformer.layer.2.output_layer_norm.bias\", \"distill_bert.transformer.layer.3.attention.q_lin.weight\", \"distill_bert.transformer.layer.3.attention.q_lin.bias\", \"distill_bert.transformer.layer.3.attention.k_lin.weight\", \"distill_bert.transformer.layer.3.attention.k_lin.bias\", \"distill_bert.transformer.layer.3.attention.v_lin.weight\", \"distill_bert.transformer.layer.3.attention.v_lin.bias\", \"distill_bert.transformer.layer.3.attention.out_lin.weight\", \"distill_bert.transformer.layer.3.attention.out_lin.bias\", \"distill_bert.transformer.layer.3.sa_layer_norm.weight\", \"distill_bert.transformer.layer.3.sa_layer_norm.bias\", \"distill_bert.transformer.layer.3.ffn.lin1.weight\", \"distill_bert.transformer.layer.3.ffn.lin1.bias\", \"distill_bert.transformer.layer.3.ffn.lin2.weight\", \"distill_bert.transformer.layer.3.ffn.lin2.bias\", \"distill_bert.transformer.layer.3.output_layer_norm.weight\", \"distill_bert.transformer.layer.3.output_layer_norm.bias\", \"distill_bert.transformer.layer.4.attention.q_lin.weight\", \"distill_bert.transformer.layer.4.attention.q_lin.bias\", \"distill_bert.transformer.layer.4.attention.k_lin.weight\", \"distill_bert.transformer.layer.4.attention.k_lin.bias\", \"distill_bert.transformer.layer.4.attention.v_lin.weight\", \"distill_bert.transformer.layer.4.attention.v_lin.bias\", \"distill_bert.transformer.layer.4.attention.out_lin.weight\", \"distill_bert.transformer.layer.4.attention.out_lin.bias\", \"distill_bert.transformer.layer.4.sa_layer_norm.weight\", \"distill_bert.transformer.layer.4.sa_layer_norm.bias\", \"distill_bert.transformer.layer.4.ffn.lin1.weight\", \"distill_bert.transformer.layer.4.ffn.lin1.bias\", \"distill_bert.transformer.layer.4.ffn.lin2.weight\", \"distill_bert.transformer.layer.4.ffn.lin2.bias\", \"distill_bert.transformer.layer.4.output_layer_norm.weight\", \"distill_bert.transformer.layer.4.output_layer_norm.bias\", \"distill_bert.transformer.layer.5.attention.q_lin.weight\", \"distill_bert.transformer.layer.5.attention.q_lin.bias\", \"distill_bert.transformer.layer.5.attention.k_lin.weight\", \"distill_bert.transformer.layer.5.attention.k_lin.bias\", \"distill_bert.transformer.layer.5.attention.v_lin.weight\", \"distill_bert.transformer.layer.5.attention.v_lin.bias\", \"distill_bert.transformer.layer.5.attention.out_lin.weight\", \"distill_bert.transformer.layer.5.attention.out_lin.bias\", \"distill_bert.transformer.layer.5.sa_layer_norm.weight\", \"distill_bert.transformer.layer.5.sa_layer_norm.bias\", \"distill_bert.transformer.layer.5.ffn.lin1.weight\", \"distill_bert.transformer.layer.5.ffn.lin1.bias\", \"distill_bert.transformer.layer.5.ffn.lin2.weight\", \"distill_bert.transformer.layer.5.ffn.lin2.bias\", \"distill_bert.transformer.layer.5.output_layer_norm.weight\", \"distill_bert.transformer.layer.5.output_layer_norm.bias\". \n\tUnexpected key(s) in state_dict: \"distill_bert.encoder.layer.0.attention.self.query.weight\", \"distill_bert.encoder.layer.0.attention.self.query.bias\", \"distill_bert.encoder.layer.0.attention.self.key.weight\", \"distill_bert.encoder.layer.0.attention.self.key.bias\", \"distill_bert.encoder.layer.0.attention.self.value.weight\", \"distill_bert.encoder.layer.0.attention.self.value.bias\", \"distill_bert.encoder.layer.0.attention.output.dense.weight\", \"distill_bert.encoder.layer.0.attention.output.dense.bias\", \"distill_bert.encoder.layer.0.attention.output.LayerNorm.weight\", \"distill_bert.encoder.layer.0.attention.output.LayerNorm.bias\", \"distill_bert.encoder.layer.0.intermediate.dense.weight\", \"distill_bert.encoder.layer.0.intermediate.dense.bias\", \"distill_bert.encoder.layer.0.output.dense.weight\", \"distill_bert.encoder.layer.0.output.dense.bias\", \"distill_bert.encoder.layer.0.output.LayerNorm.weight\", \"distill_bert.encoder.layer.0.output.LayerNorm.bias\", \"distill_bert.encoder.layer.1.attention.self.query.weight\", \"distill_bert.encoder.layer.1.attention.self.query.bias\", \"distill_bert.encoder.layer.1.attention.self.key.weight\", \"distill_bert.encoder.layer.1.attention.self.key.bias\", \"distill_bert.encoder.layer.1.attention.self.value.weight\", \"distill_bert.encoder.layer.1.attention.self.value.bias\", \"distill_bert.encoder.layer.1.attention.output.dense.weight\", \"distill_bert.encoder.layer.1.attention.output.dense.bias\", \"distill_bert.encoder.layer.1.attention.output.LayerNorm.weight\", \"distill_bert.encoder.layer.1.attention.output.LayerNorm.bias\", \"distill_bert.encoder.layer.1.intermediate.dense.weight\", \"distill_bert.encoder.layer.1.intermediate.dense.bias\", \"distill_bert.encoder.layer.1.output.dense.weight\", \"distill_bert.encoder.layer.1.output.dense.bias\", \"distill_bert.encoder.layer.1.output.LayerNorm.weight\", \"distill_bert.encoder.layer.1.output.LayerNorm.bias\", \"distill_bert.encoder.layer.2.attention.self.query.weight\", \"distill_bert.encoder.layer.2.attention.self.query.bias\", \"distill_bert.encoder.layer.2.attention.self.key.weight\", \"distill_bert.encoder.layer.2.attention.self.key.bias\", \"distill_bert.encoder.layer.2.attention.self.value.weight\", \"distill_bert.encoder.layer.2.attention.self.value.bias\", \"distill_bert.encoder.layer.2.attention.output.dense.weight\", \"distill_bert.encoder.layer.2.attention.output.dense.bias\", \"distill_bert.encoder.layer.2.attention.output.LayerNorm.weight\", \"distill_bert.encoder.layer.2.attention.output.LayerNorm.bias\", \"distill_bert.encoder.layer.2.intermediate.dense.weight\", \"distill_bert.encoder.layer.2.intermediate.dense.bias\", \"distill_bert.encoder.layer.2.output.dense.weight\", \"distill_bert.encoder.layer.2.output.dense.bias\", \"distill_bert.encoder.layer.2.output.LayerNorm.weight\", \"distill_bert.encoder.layer.2.output.LayerNorm.bias\", \"distill_bert.encoder.layer.3.attention.self.query.weight\", \"distill_bert.encoder.layer.3.attention.self.query.bias\", \"distill_bert.encoder.layer.3.attention.self.key.weight\", \"distill_bert.encoder.layer.3.attention.self.key.bias\", \"distill_bert.encoder.layer.3.attention.self.value.weight\", \"distill_bert.encoder.layer.3.attention.self.value.bias\", \"distill_bert.encoder.layer.3.attention.output.dense.weight\", \"distill_bert.encoder.layer.3.attention.output.dense.bias\", \"distill_bert.encoder.layer.3.attention.output.LayerNorm.weight\", \"distill_bert.encoder.layer.3.attention.output.LayerNorm.bias\", \"distill_bert.encoder.layer.3.intermediate.dense.weight\", \"distill_bert.encoder.layer.3.intermediate.dense.bias\", \"distill_bert.encoder.layer.3.output.dense.weight\", \"distill_bert.encoder.layer.3.output.dense.bias\", \"distill_bert.encoder.layer.3.output.LayerNorm.weight\", \"distill_bert.encoder.layer.3.output.LayerNorm.bias\", \"distill_bert.encoder.layer.4.attention.self.query.weight\", \"distill_bert.encoder.layer.4.attention.self.query.bias\", \"distill_bert.encoder.layer.4.attention.self.key.weight\", \"distill_bert.encoder.layer.4.attention.self.key.bias\", \"distill_bert.encoder.layer.4.attention.self.value.weight\", \"distill_bert.encoder.layer.4.attention.self.value.bias\", \"distill_bert.encoder.layer.4.attention.output.dense.weight\", \"distill_bert.encoder.layer.4.attention.output.dense.bias\", \"distill_bert.encoder.layer.4.attention.output.LayerNorm.weight\", \"distill_bert.encoder.layer.4.attention.output.LayerNorm.bias\", \"distill_bert.encoder.layer.4.intermediate.dense.weight\", \"distill_bert.encoder.layer.4.intermediate.dense.bias\", \"distill_bert.encoder.layer.4.output.dense.weight\", \"distill_bert.encoder.layer.4.output.dense.bias\", \"distill_bert.encoder.layer.4.output.LayerNorm.weight\", \"distill_bert.encoder.layer.4.output.LayerNorm.bias\", \"distill_bert.encoder.layer.5.attention.self.query.weight\", \"distill_bert.encoder.layer.5.attention.self.query.bias\", \"distill_bert.encoder.layer.5.attention.self.key.weight\", \"distill_bert.encoder.layer.5.attention.self.key.bias\", \"distill_bert.encoder.layer.5.attention.self.value.weight\", \"distill_bert.encoder.layer.5.attention.self.value.bias\", \"distill_bert.encoder.layer.5.attention.output.dense.weight\", \"distill_bert.encoder.layer.5.attention.output.dense.bias\", \"distill_bert.encoder.layer.5.attention.output.LayerNorm.weight\", \"distill_bert.encoder.layer.5.attention.output.LayerNorm.bias\", \"distill_bert.encoder.layer.5.intermediate.dense.weight\", \"distill_bert.encoder.layer.5.intermediate.dense.bias\", \"distill_bert.encoder.layer.5.output.dense.weight\", \"distill_bert.encoder.layer.5.output.dense.bias\", \"distill_bert.encoder.layer.5.output.LayerNorm.weight\", \"distill_bert.encoder.layer.5.output.LayerNorm.bias\", \"distill_bert.encoder.layer.6.attention.self.query.weight\", \"distill_bert.encoder.layer.6.attention.self.query.bias\", \"distill_bert.encoder.layer.6.attention.self.key.weight\", \"distill_bert.encoder.layer.6.attention.self.key.bias\", \"distill_bert.encoder.layer.6.attention.self.value.weight\", \"distill_bert.encoder.layer.6.attention.self.value.bias\", \"distill_bert.encoder.layer.6.attention.output.dense.weight\", \"distill_bert.encoder.layer.6.attention.output.dense.bias\", \"distill_bert.encoder.layer.6.attention.output.LayerNorm.weight\", \"distill_bert.encoder.layer.6.attention.output.LayerNorm.bias\", \"distill_bert.encoder.layer.6.intermediate.dense.weight\", \"distill_bert.encoder.layer.6.intermediate.dense.bias\", \"distill_bert.encoder.layer.6.output.dense.weight\", \"distill_bert.encoder.layer.6.output.dense.bias\", \"distill_bert.encoder.layer.6.output.LayerNorm.weight\", \"distill_bert.encoder.layer.6.output.LayerNorm.bias\", \"distill_bert.encoder.layer.7.attention.self.query.weight\", \"distill_bert.encoder.layer.7.attention.self.query.bias\", \"distill_bert.encoder.layer.7.attention.self.key.weight\", \"distill_bert.encoder.layer.7.attention.self.key.bias\", \"distill_bert.encoder.layer.7.attention.self.value.weight\", \"distill_bert.encoder.layer.7.attention.self.value.bias\", \"distill_bert.encoder.layer.7.attention.output.dense.weight\", \"distill_bert.encoder.layer.7.attention.output.dense.bias\", \"distill_bert.encoder.layer.7.attention.output.LayerNorm.weight\", \"distill_bert.encoder.layer.7.attention.output.LayerNorm.bias\", \"distill_bert.encoder.layer.7.intermediate.dense.weight\", \"distill_bert.encoder.layer.7.intermediate.dense.bias\", \"distill_bert.encoder.layer.7.output.dense.weight\", \"distill_bert.encoder.layer.7.output.dense.bias\", \"distill_bert.encoder.layer.7.output.LayerNorm.weight\", \"distill_bert.encoder.layer.7.output.LayerNorm.bias\", \"distill_bert.encoder.layer.8.attention.self.query.weight\", \"distill_bert.encoder.layer.8.attention.self.query.bias\", \"distill_bert.encoder.layer.8.attention.self.key.weight\", \"distill_bert.encoder.layer.8.attention.self.key.bias\", \"distill_bert.encoder.layer.8.attention.self.value.weight\", \"distill_bert.encoder.layer.8.attention.self.value.bias\", \"distill_bert.encoder.layer.8.attention.output.dense.weight\", \"distill_bert.encoder.layer.8.attention.output.dense.bias\", \"distill_bert.encoder.layer.8.attention.output.LayerNorm.weight\", \"distill_bert.encoder.layer.8.attention.output.LayerNorm.bias\", \"distill_bert.encoder.layer.8.intermediate.dense.weight\", \"distill_bert.encoder.layer.8.intermediate.dense.bias\", \"distill_bert.encoder.layer.8.output.dense.weight\", \"distill_bert.encoder.layer.8.output.dense.bias\", \"distill_bert.encoder.layer.8.output.LayerNorm.weight\", \"distill_bert.encoder.layer.8.output.LayerNorm.bias\", \"distill_bert.encoder.layer.9.attention.self.query.weight\", \"distill_bert.encoder.layer.9.attention.self.query.bias\", \"distill_bert.encoder.layer.9.attention.self.key.weight\", \"distill_bert.encoder.layer.9.attention.self.key.bias\", \"distill_bert.encoder.layer.9.attention.self.value.weight\", \"distill_bert.encoder.layer.9.attention.self.value.bias\", \"distill_bert.encoder.layer.9.attention.output.dense.weight\", \"distill_bert.encoder.layer.9.attention.output.dense.bias\", \"distill_bert.encoder.layer.9.attention.output.LayerNorm.weight\", \"distill_bert.encoder.layer.9.attention.output.LayerNorm.bias\", \"distill_bert.encoder.layer.9.intermediate.dense.weight\", \"distill_bert.encoder.layer.9.intermediate.dense.bias\", \"distill_bert.encoder.layer.9.output.dense.weight\", \"distill_bert.encoder.layer.9.output.dense.bias\", \"distill_bert.encoder.layer.9.output.LayerNorm.weight\", \"distill_bert.encoder.layer.9.output.LayerNorm.bias\", \"distill_bert.encoder.layer.10.attention.self.query.weight\", \"distill_bert.encoder.layer.10.attention.self.query.bias\", \"distill_bert.encoder.layer.10.attention.self.key.weight\", \"distill_bert.encoder.layer.10.attention.self.key.bias\", \"distill_bert.encoder.layer.10.attention.self.value.weight\", \"distill_bert.encoder.layer.10.attention.self.value.bias\", \"distill_bert.encoder.layer.10.attention.output.dense.weight\", \"distill_bert.encoder.layer.10.attention.output.dense.bias\", \"distill_bert.encoder.layer.10.attention.output.LayerNorm.weight\", \"distill_bert.encoder.layer.10.attention.output.LayerNorm.bias\", \"distill_bert.encoder.layer.10.intermediate.dense.weight\", \"distill_bert.encoder.layer.10.intermediate.dense.bias\", \"distill_bert.encoder.layer.10.output.dense.weight\", \"distill_bert.encoder.layer.10.output.dense.bias\", \"distill_bert.encoder.layer.10.output.LayerNorm.weight\", \"distill_bert.encoder.layer.10.output.LayerNorm.bias\", \"distill_bert.encoder.layer.11.attention.self.query.weight\", \"distill_bert.encoder.layer.11.attention.self.query.bias\", \"distill_bert.encoder.layer.11.attention.self.key.weight\", \"distill_bert.encoder.layer.11.attention.self.key.bias\", \"distill_bert.encoder.layer.11.attention.self.value.weight\", \"distill_bert.encoder.layer.11.attention.self.value.bias\", \"distill_bert.encoder.layer.11.attention.output.dense.weight\", \"distill_bert.encoder.layer.11.attention.output.dense.bias\", \"distill_bert.encoder.layer.11.attention.output.LayerNorm.weight\", \"distill_bert.encoder.layer.11.attention.output.LayerNorm.bias\", \"distill_bert.encoder.layer.11.intermediate.dense.weight\", \"distill_bert.encoder.layer.11.intermediate.dense.bias\", \"distill_bert.encoder.layer.11.output.dense.weight\", \"distill_bert.encoder.layer.11.output.dense.bias\", \"distill_bert.encoder.layer.11.output.LayerNorm.weight\", \"distill_bert.encoder.layer.11.output.LayerNorm.bias\", \"distill_bert.pooler.dense.weight\", \"distill_bert.pooler.dense.bias\", \"distill_bert.embeddings.position_ids\", \"distill_bert.embeddings.token_type_embeddings.weight\". \n\tsize mismatch for distill_bert.embeddings.word_embeddings.weight: copying a param with shape torch.Size([50265, 768]) from checkpoint, the shape in current model is torch.Size([30522, 768]).\n\tsize mismatch for distill_bert.embeddings.position_embeddings.weight: copying a param with shape torch.Size([514, 768]) from checkpoint, the shape in current model is torch.Size([512, 768]).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[1;32mIn [14]\u001b[0m, in \u001b[0;36m<cell line: 13>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      7\u001b[0m BATCH_SIZE \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m8\u001b[39m\n\u001b[0;32m      8\u001b[0m TOTAL_MAX_LEN \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m256\u001b[39m\n\u001b[1;32m---> 13\u001b[0m code_df_valid[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrank\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mBERT_MODEL_NAME\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcheck_point\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodels_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcode_bert_checkpoint.pt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mBATCH_SIZE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_workers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mPROCESSORS_COUNT\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_len\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mCD_MAX_LENGTH\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgenerated_columns_count\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mGENERATED_COLUMNS_COUNT\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtotal_max_len\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mTOTAL_MAX_LEN\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcode_df_valid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     22\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdrop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mMODEL_USELESS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     23\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     25\u001b[0m mkdn_df_valid[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrank\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m m\u001b[38;5;241m.\u001b[39mpredict(\n\u001b[0;32m     26\u001b[0m     model_path\u001b[38;5;241m=\u001b[39mBERT_MODEL_NAME,\n\u001b[0;32m     27\u001b[0m     check_point\u001b[38;5;241m=\u001b[39mos\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(models_dir, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmarkdown_bert_checkpoint.pt\u001b[39m\u001b[38;5;124m'\u001b[39m), \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     34\u001b[0m     drop\u001b[38;5;241m=\u001b[39mMODEL_USELESS\n\u001b[0;32m     35\u001b[0m )\n\u001b[0;32m     37\u001b[0m vres_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat([mkdn_df_valid, code_df_valid], ignore_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, )\u001b[38;5;241m.\u001b[39msort_values(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrank\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mgroupby(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcell_id\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28mlist\u001b[39m)\n",
      "File \u001b[1;32mE:\\AI4Code\\pt_model.py:511\u001b[0m, in \u001b[0;36mpredict\u001b[1;34m(model_path, check_point, batch_size, num_workers, max_len, generated_columns_count, total_max_len, data, drop)\u001b[0m\n\u001b[0;32m    509\u001b[0m model \u001b[38;5;241m=\u001b[39m BModel(model_path, generated_columns_count)\u001b[38;5;241m.\u001b[39mcuda()\n\u001b[0;32m    510\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m--> 511\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcheck_point\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    514\u001b[0m \u001b[38;5;66;03m# prepare the data loaders \u001b[39;00m\n\u001b[0;32m    515\u001b[0m data_ds \u001b[38;5;241m=\u001b[39m BDataset(\n\u001b[0;32m    516\u001b[0m     data, \n\u001b[0;32m    517\u001b[0m     max_len\u001b[38;5;241m=\u001b[39mmax_len, \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    520\u001b[0m     drop\u001b[38;5;241m=\u001b[39mdrop\n\u001b[0;32m    521\u001b[0m )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1497\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[1;34m(self, state_dict, strict)\u001b[0m\n\u001b[0;32m   1492\u001b[0m         error_msgs\u001b[38;5;241m.\u001b[39minsert(\n\u001b[0;32m   1493\u001b[0m             \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m   1494\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(k) \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)))\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m-> 1497\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m   1498\u001b[0m                        \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(error_msgs)))\n\u001b[0;32m   1499\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for BModel:\n\tMissing key(s) in state_dict: \"distill_bert.transformer.layer.0.attention.q_lin.weight\", \"distill_bert.transformer.layer.0.attention.q_lin.bias\", \"distill_bert.transformer.layer.0.attention.k_lin.weight\", \"distill_bert.transformer.layer.0.attention.k_lin.bias\", \"distill_bert.transformer.layer.0.attention.v_lin.weight\", \"distill_bert.transformer.layer.0.attention.v_lin.bias\", \"distill_bert.transformer.layer.0.attention.out_lin.weight\", \"distill_bert.transformer.layer.0.attention.out_lin.bias\", \"distill_bert.transformer.layer.0.sa_layer_norm.weight\", \"distill_bert.transformer.layer.0.sa_layer_norm.bias\", \"distill_bert.transformer.layer.0.ffn.lin1.weight\", \"distill_bert.transformer.layer.0.ffn.lin1.bias\", \"distill_bert.transformer.layer.0.ffn.lin2.weight\", \"distill_bert.transformer.layer.0.ffn.lin2.bias\", \"distill_bert.transformer.layer.0.output_layer_norm.weight\", \"distill_bert.transformer.layer.0.output_layer_norm.bias\", \"distill_bert.transformer.layer.1.attention.q_lin.weight\", \"distill_bert.transformer.layer.1.attention.q_lin.bias\", \"distill_bert.transformer.layer.1.attention.k_lin.weight\", \"distill_bert.transformer.layer.1.attention.k_lin.bias\", \"distill_bert.transformer.layer.1.attention.v_lin.weight\", \"distill_bert.transformer.layer.1.attention.v_lin.bias\", \"distill_bert.transformer.layer.1.attention.out_lin.weight\", \"distill_bert.transformer.layer.1.attention.out_lin.bias\", \"distill_bert.transformer.layer.1.sa_layer_norm.weight\", \"distill_bert.transformer.layer.1.sa_layer_norm.bias\", \"distill_bert.transformer.layer.1.ffn.lin1.weight\", \"distill_bert.transformer.layer.1.ffn.lin1.bias\", \"distill_bert.transformer.layer.1.ffn.lin2.weight\", \"distill_bert.transformer.layer.1.ffn.lin2.bias\", \"distill_bert.transformer.layer.1.output_layer_norm.weight\", \"distill_bert.transformer.layer.1.output_layer_norm.bias\", \"distill_bert.transformer.layer.2.attention.q_lin.weight\", \"distill_bert.transformer.layer.2.attention.q_lin.bias\", \"distill_bert.transformer.layer.2.attention.k_lin.weight\", \"distill_bert.transformer.layer.2.attention.k_lin.bias\", \"distill_bert.transformer.layer.2.attention.v_lin.weight\", \"distill_bert.transformer.layer.2.attention.v_lin.bias\", \"distill_bert.transformer.layer.2.attention.out_lin.weight\", \"distill_bert.transformer.layer.2.attention.out_lin.bias\", \"distill_bert.transformer.layer.2.sa_layer_norm.weight\", \"distill_bert.transformer.layer.2.sa_layer_norm.bias\", \"distill_bert.transformer.layer.2.ffn.lin1.weight\", \"distill_bert.transformer.layer.2.ffn.lin1.bias\", \"distill_bert.transformer.layer.2.ffn.lin2.weight\", \"distill_bert.transformer.layer.2.ffn.lin2.bias\", \"distill_bert.transformer.layer.2.output_layer_norm.weight\", \"distill_bert.transformer.layer.2.output_layer_norm.bias\", \"distill_bert.transformer.layer.3.attention.q_lin.weight\", \"distill_bert.transformer.layer.3.attention.q_lin.bias\", \"distill_bert.transformer.layer.3.attention.k_lin.weight\", \"distill_bert.transformer.layer.3.attention.k_lin.bias\", \"distill_bert.transformer.layer.3.attention.v_lin.weight\", \"distill_bert.transformer.layer.3.attention.v_lin.bias\", \"distill_bert.transformer.layer.3.attention.out_lin.weight\", \"distill_bert.transformer.layer.3.attention.out_lin.bias\", \"distill_bert.transformer.layer.3.sa_layer_norm.weight\", \"distill_bert.transformer.layer.3.sa_layer_norm.bias\", \"distill_bert.transformer.layer.3.ffn.lin1.weight\", \"distill_bert.transformer.layer.3.ffn.lin1.bias\", \"distill_bert.transformer.layer.3.ffn.lin2.weight\", \"distill_bert.transformer.layer.3.ffn.lin2.bias\", \"distill_bert.transformer.layer.3.output_layer_norm.weight\", \"distill_bert.transformer.layer.3.output_layer_norm.bias\", \"distill_bert.transformer.layer.4.attention.q_lin.weight\", \"distill_bert.transformer.layer.4.attention.q_lin.bias\", \"distill_bert.transformer.layer.4.attention.k_lin.weight\", \"distill_bert.transformer.layer.4.attention.k_lin.bias\", \"distill_bert.transformer.layer.4.attention.v_lin.weight\", \"distill_bert.transformer.layer.4.attention.v_lin.bias\", \"distill_bert.transformer.layer.4.attention.out_lin.weight\", \"distill_bert.transformer.layer.4.attention.out_lin.bias\", \"distill_bert.transformer.layer.4.sa_layer_norm.weight\", \"distill_bert.transformer.layer.4.sa_layer_norm.bias\", \"distill_bert.transformer.layer.4.ffn.lin1.weight\", \"distill_bert.transformer.layer.4.ffn.lin1.bias\", \"distill_bert.transformer.layer.4.ffn.lin2.weight\", \"distill_bert.transformer.layer.4.ffn.lin2.bias\", \"distill_bert.transformer.layer.4.output_layer_norm.weight\", \"distill_bert.transformer.layer.4.output_layer_norm.bias\", \"distill_bert.transformer.layer.5.attention.q_lin.weight\", \"distill_bert.transformer.layer.5.attention.q_lin.bias\", \"distill_bert.transformer.layer.5.attention.k_lin.weight\", \"distill_bert.transformer.layer.5.attention.k_lin.bias\", \"distill_bert.transformer.layer.5.attention.v_lin.weight\", \"distill_bert.transformer.layer.5.attention.v_lin.bias\", \"distill_bert.transformer.layer.5.attention.out_lin.weight\", \"distill_bert.transformer.layer.5.attention.out_lin.bias\", \"distill_bert.transformer.layer.5.sa_layer_norm.weight\", \"distill_bert.transformer.layer.5.sa_layer_norm.bias\", \"distill_bert.transformer.layer.5.ffn.lin1.weight\", \"distill_bert.transformer.layer.5.ffn.lin1.bias\", \"distill_bert.transformer.layer.5.ffn.lin2.weight\", \"distill_bert.transformer.layer.5.ffn.lin2.bias\", \"distill_bert.transformer.layer.5.output_layer_norm.weight\", \"distill_bert.transformer.layer.5.output_layer_norm.bias\". \n\tUnexpected key(s) in state_dict: \"distill_bert.encoder.layer.0.attention.self.query.weight\", \"distill_bert.encoder.layer.0.attention.self.query.bias\", \"distill_bert.encoder.layer.0.attention.self.key.weight\", \"distill_bert.encoder.layer.0.attention.self.key.bias\", \"distill_bert.encoder.layer.0.attention.self.value.weight\", \"distill_bert.encoder.layer.0.attention.self.value.bias\", \"distill_bert.encoder.layer.0.attention.output.dense.weight\", \"distill_bert.encoder.layer.0.attention.output.dense.bias\", \"distill_bert.encoder.layer.0.attention.output.LayerNorm.weight\", \"distill_bert.encoder.layer.0.attention.output.LayerNorm.bias\", \"distill_bert.encoder.layer.0.intermediate.dense.weight\", \"distill_bert.encoder.layer.0.intermediate.dense.bias\", \"distill_bert.encoder.layer.0.output.dense.weight\", \"distill_bert.encoder.layer.0.output.dense.bias\", \"distill_bert.encoder.layer.0.output.LayerNorm.weight\", \"distill_bert.encoder.layer.0.output.LayerNorm.bias\", \"distill_bert.encoder.layer.1.attention.self.query.weight\", \"distill_bert.encoder.layer.1.attention.self.query.bias\", \"distill_bert.encoder.layer.1.attention.self.key.weight\", \"distill_bert.encoder.layer.1.attention.self.key.bias\", \"distill_bert.encoder.layer.1.attention.self.value.weight\", \"distill_bert.encoder.layer.1.attention.self.value.bias\", \"distill_bert.encoder.layer.1.attention.output.dense.weight\", \"distill_bert.encoder.layer.1.attention.output.dense.bias\", \"distill_bert.encoder.layer.1.attention.output.LayerNorm.weight\", \"distill_bert.encoder.layer.1.attention.output.LayerNorm.bias\", \"distill_bert.encoder.layer.1.intermediate.dense.weight\", \"distill_bert.encoder.layer.1.intermediate.dense.bias\", \"distill_bert.encoder.layer.1.output.dense.weight\", \"distill_bert.encoder.layer.1.output.dense.bias\", \"distill_bert.encoder.layer.1.output.LayerNorm.weight\", \"distill_bert.encoder.layer.1.output.LayerNorm.bias\", \"distill_bert.encoder.layer.2.attention.self.query.weight\", \"distill_bert.encoder.layer.2.attention.self.query.bias\", \"distill_bert.encoder.layer.2.attention.self.key.weight\", \"distill_bert.encoder.layer.2.attention.self.key.bias\", \"distill_bert.encoder.layer.2.attention.self.value.weight\", \"distill_bert.encoder.layer.2.attention.self.value.bias\", \"distill_bert.encoder.layer.2.attention.output.dense.weight\", \"distill_bert.encoder.layer.2.attention.output.dense.bias\", \"distill_bert.encoder.layer.2.attention.output.LayerNorm.weight\", \"distill_bert.encoder.layer.2.attention.output.LayerNorm.bias\", \"distill_bert.encoder.layer.2.intermediate.dense.weight\", \"distill_bert.encoder.layer.2.intermediate.dense.bias\", \"distill_bert.encoder.layer.2.output.dense.weight\", \"distill_bert.encoder.layer.2.output.dense.bias\", \"distill_bert.encoder.layer.2.output.LayerNorm.weight\", \"distill_bert.encoder.layer.2.output.LayerNorm.bias\", \"distill_bert.encoder.layer.3.attention.self.query.weight\", \"distill_bert.encoder.layer.3.attention.self.query.bias\", \"distill_bert.encoder.layer.3.attention.self.key.weight\", \"distill_bert.encoder.layer.3.attention.self.key.bias\", \"distill_bert.encoder.layer.3.attention.self.value.weight\", \"distill_bert.encoder.layer.3.attention.self.value.bias\", \"distill_bert.encoder.layer.3.attention.output.dense.weight\", \"distill_bert.encoder.layer.3.attention.output.dense.bias\", \"distill_bert.encoder.layer.3.attention.output.LayerNorm.weight\", \"distill_bert.encoder.layer.3.attention.output.LayerNorm.bias\", \"distill_bert.encoder.layer.3.intermediate.dense.weight\", \"distill_bert.encoder.layer.3.intermediate.dense.bias\", \"distill_bert.encoder.layer.3.output.dense.weight\", \"distill_bert.encoder.layer.3.output.dense.bias\", \"distill_bert.encoder.layer.3.output.LayerNorm.weight\", \"distill_bert.encoder.layer.3.output.LayerNorm.bias\", \"distill_bert.encoder.layer.4.attention.self.query.weight\", \"distill_bert.encoder.layer.4.attention.self.query.bias\", \"distill_bert.encoder.layer.4.attention.self.key.weight\", \"distill_bert.encoder.layer.4.attention.self.key.bias\", \"distill_bert.encoder.layer.4.attention.self.value.weight\", \"distill_bert.encoder.layer.4.attention.self.value.bias\", \"distill_bert.encoder.layer.4.attention.output.dense.weight\", \"distill_bert.encoder.layer.4.attention.output.dense.bias\", \"distill_bert.encoder.layer.4.attention.output.LayerNorm.weight\", \"distill_bert.encoder.layer.4.attention.output.LayerNorm.bias\", \"distill_bert.encoder.layer.4.intermediate.dense.weight\", \"distill_bert.encoder.layer.4.intermediate.dense.bias\", \"distill_bert.encoder.layer.4.output.dense.weight\", \"distill_bert.encoder.layer.4.output.dense.bias\", \"distill_bert.encoder.layer.4.output.LayerNorm.weight\", \"distill_bert.encoder.layer.4.output.LayerNorm.bias\", \"distill_bert.encoder.layer.5.attention.self.query.weight\", \"distill_bert.encoder.layer.5.attention.self.query.bias\", \"distill_bert.encoder.layer.5.attention.self.key.weight\", \"distill_bert.encoder.layer.5.attention.self.key.bias\", \"distill_bert.encoder.layer.5.attention.self.value.weight\", \"distill_bert.encoder.layer.5.attention.self.value.bias\", \"distill_bert.encoder.layer.5.attention.output.dense.weight\", \"distill_bert.encoder.layer.5.attention.output.dense.bias\", \"distill_bert.encoder.layer.5.attention.output.LayerNorm.weight\", \"distill_bert.encoder.layer.5.attention.output.LayerNorm.bias\", \"distill_bert.encoder.layer.5.intermediate.dense.weight\", \"distill_bert.encoder.layer.5.intermediate.dense.bias\", \"distill_bert.encoder.layer.5.output.dense.weight\", \"distill_bert.encoder.layer.5.output.dense.bias\", \"distill_bert.encoder.layer.5.output.LayerNorm.weight\", \"distill_bert.encoder.layer.5.output.LayerNorm.bias\", \"distill_bert.encoder.layer.6.attention.self.query.weight\", \"distill_bert.encoder.layer.6.attention.self.query.bias\", \"distill_bert.encoder.layer.6.attention.self.key.weight\", \"distill_bert.encoder.layer.6.attention.self.key.bias\", \"distill_bert.encoder.layer.6.attention.self.value.weight\", \"distill_bert.encoder.layer.6.attention.self.value.bias\", \"distill_bert.encoder.layer.6.attention.output.dense.weight\", \"distill_bert.encoder.layer.6.attention.output.dense.bias\", \"distill_bert.encoder.layer.6.attention.output.LayerNorm.weight\", \"distill_bert.encoder.layer.6.attention.output.LayerNorm.bias\", \"distill_bert.encoder.layer.6.intermediate.dense.weight\", \"distill_bert.encoder.layer.6.intermediate.dense.bias\", \"distill_bert.encoder.layer.6.output.dense.weight\", \"distill_bert.encoder.layer.6.output.dense.bias\", \"distill_bert.encoder.layer.6.output.LayerNorm.weight\", \"distill_bert.encoder.layer.6.output.LayerNorm.bias\", \"distill_bert.encoder.layer.7.attention.self.query.weight\", \"distill_bert.encoder.layer.7.attention.self.query.bias\", \"distill_bert.encoder.layer.7.attention.self.key.weight\", \"distill_bert.encoder.layer.7.attention.self.key.bias\", \"distill_bert.encoder.layer.7.attention.self.value.weight\", \"distill_bert.encoder.layer.7.attention.self.value.bias\", \"distill_bert.encoder.layer.7.attention.output.dense.weight\", \"distill_bert.encoder.layer.7.attention.output.dense.bias\", \"distill_bert.encoder.layer.7.attention.output.LayerNorm.weight\", \"distill_bert.encoder.layer.7.attention.output.LayerNorm.bias\", \"distill_bert.encoder.layer.7.intermediate.dense.weight\", \"distill_bert.encoder.layer.7.intermediate.dense.bias\", \"distill_bert.encoder.layer.7.output.dense.weight\", \"distill_bert.encoder.layer.7.output.dense.bias\", \"distill_bert.encoder.layer.7.output.LayerNorm.weight\", \"distill_bert.encoder.layer.7.output.LayerNorm.bias\", \"distill_bert.encoder.layer.8.attention.self.query.weight\", \"distill_bert.encoder.layer.8.attention.self.query.bias\", \"distill_bert.encoder.layer.8.attention.self.key.weight\", \"distill_bert.encoder.layer.8.attention.self.key.bias\", \"distill_bert.encoder.layer.8.attention.self.value.weight\", \"distill_bert.encoder.layer.8.attention.self.value.bias\", \"distill_bert.encoder.layer.8.attention.output.dense.weight\", \"distill_bert.encoder.layer.8.attention.output.dense.bias\", \"distill_bert.encoder.layer.8.attention.output.LayerNorm.weight\", \"distill_bert.encoder.layer.8.attention.output.LayerNorm.bias\", \"distill_bert.encoder.layer.8.intermediate.dense.weight\", \"distill_bert.encoder.layer.8.intermediate.dense.bias\", \"distill_bert.encoder.layer.8.output.dense.weight\", \"distill_bert.encoder.layer.8.output.dense.bias\", \"distill_bert.encoder.layer.8.output.LayerNorm.weight\", \"distill_bert.encoder.layer.8.output.LayerNorm.bias\", \"distill_bert.encoder.layer.9.attention.self.query.weight\", \"distill_bert.encoder.layer.9.attention.self.query.bias\", \"distill_bert.encoder.layer.9.attention.self.key.weight\", \"distill_bert.encoder.layer.9.attention.self.key.bias\", \"distill_bert.encoder.layer.9.attention.self.value.weight\", \"distill_bert.encoder.layer.9.attention.self.value.bias\", \"distill_bert.encoder.layer.9.attention.output.dense.weight\", \"distill_bert.encoder.layer.9.attention.output.dense.bias\", \"distill_bert.encoder.layer.9.attention.output.LayerNorm.weight\", \"distill_bert.encoder.layer.9.attention.output.LayerNorm.bias\", \"distill_bert.encoder.layer.9.intermediate.dense.weight\", \"distill_bert.encoder.layer.9.intermediate.dense.bias\", \"distill_bert.encoder.layer.9.output.dense.weight\", \"distill_bert.encoder.layer.9.output.dense.bias\", \"distill_bert.encoder.layer.9.output.LayerNorm.weight\", \"distill_bert.encoder.layer.9.output.LayerNorm.bias\", \"distill_bert.encoder.layer.10.attention.self.query.weight\", \"distill_bert.encoder.layer.10.attention.self.query.bias\", \"distill_bert.encoder.layer.10.attention.self.key.weight\", \"distill_bert.encoder.layer.10.attention.self.key.bias\", \"distill_bert.encoder.layer.10.attention.self.value.weight\", \"distill_bert.encoder.layer.10.attention.self.value.bias\", \"distill_bert.encoder.layer.10.attention.output.dense.weight\", \"distill_bert.encoder.layer.10.attention.output.dense.bias\", \"distill_bert.encoder.layer.10.attention.output.LayerNorm.weight\", \"distill_bert.encoder.layer.10.attention.output.LayerNorm.bias\", \"distill_bert.encoder.layer.10.intermediate.dense.weight\", \"distill_bert.encoder.layer.10.intermediate.dense.bias\", \"distill_bert.encoder.layer.10.output.dense.weight\", \"distill_bert.encoder.layer.10.output.dense.bias\", \"distill_bert.encoder.layer.10.output.LayerNorm.weight\", \"distill_bert.encoder.layer.10.output.LayerNorm.bias\", \"distill_bert.encoder.layer.11.attention.self.query.weight\", \"distill_bert.encoder.layer.11.attention.self.query.bias\", \"distill_bert.encoder.layer.11.attention.self.key.weight\", \"distill_bert.encoder.layer.11.attention.self.key.bias\", \"distill_bert.encoder.layer.11.attention.self.value.weight\", \"distill_bert.encoder.layer.11.attention.self.value.bias\", \"distill_bert.encoder.layer.11.attention.output.dense.weight\", \"distill_bert.encoder.layer.11.attention.output.dense.bias\", \"distill_bert.encoder.layer.11.attention.output.LayerNorm.weight\", \"distill_bert.encoder.layer.11.attention.output.LayerNorm.bias\", \"distill_bert.encoder.layer.11.intermediate.dense.weight\", \"distill_bert.encoder.layer.11.intermediate.dense.bias\", \"distill_bert.encoder.layer.11.output.dense.weight\", \"distill_bert.encoder.layer.11.output.dense.bias\", \"distill_bert.encoder.layer.11.output.LayerNorm.weight\", \"distill_bert.encoder.layer.11.output.LayerNorm.bias\", \"distill_bert.pooler.dense.weight\", \"distill_bert.pooler.dense.bias\", \"distill_bert.embeddings.position_ids\", \"distill_bert.embeddings.token_type_embeddings.weight\". \n\tsize mismatch for distill_bert.embeddings.word_embeddings.weight: copying a param with shape torch.Size([50265, 768]) from checkpoint, the shape in current model is torch.Size([30522, 768]).\n\tsize mismatch for distill_bert.embeddings.position_embeddings.weight: copying a param with shape torch.Size([514, 768]) from checkpoint, the shape in current model is torch.Size([512, 768])."
     ]
    }
   ],
   "source": [
    "CODE_TYPE = 'code'\n",
    "MKDN_TYPE = 'markdown'\n",
    "\n",
    "BERT_MODEL_NAME = 'distilbert-base-uncased'\n",
    "PROCESSORS_COUNT = psutil.cpu_count(logical=False)\n",
    "\n",
    "BATCH_SIZE = 8\n",
    "TOTAL_MAX_LEN = 256\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "code_df_valid['rank'] = m.predict(\n",
    "    model_path=BERT_MODEL_NAME,\n",
    "    check_point=os.path.join(models_dir, 'code_bert_checkpoint.pt'), \n",
    "    batch_size=BATCH_SIZE, \n",
    "    num_workers=PROCESSORS_COUNT, \n",
    "    max_len=CD_MAX_LENGTH, \n",
    "    generated_columns_count=GENERATED_COLUMNS_COUNT,\n",
    "    total_max_len=TOTAL_MAX_LEN, \n",
    "    data=code_df_valid, \n",
    "    drop=MODEL_USELESS,\n",
    ")\n",
    "\n",
    "mkdn_df_valid['rank'] = m.predict(\n",
    "    model_path=BERT_MODEL_NAME,\n",
    "    check_point=os.path.join(models_dir, 'markdown_bert_checkpoint.pt'), \n",
    "    batch_size=BATCH_SIZE, \n",
    "    num_workers=PROCESSORS_COUNT, \n",
    "    max_len=MK_MAX_LENGTH,\n",
    "    generated_columns_count=GENERATED_COLUMNS_COUNT,\n",
    "    total_max_len=TOTAL_MAX_LEN, \n",
    "    data=mkdn_df_valid, \n",
    "    drop=MODEL_USELESS\n",
    ")\n",
    "\n",
    "vres_df = pd.concat([mkdn_df_valid, code_df_valid], ignore_index=True, ).sort_values(\"rank\").groupby(\"id\")[\"cell_id\"].apply(list)\n",
    "print('Final total accuracy is:', r.kendall_tau(df_orders.loc[vres_df.index], vres_df))\n",
    "\n",
    "display(vres_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d5c0cd-8d19-4588-b19b-80e9aca01a19",
   "metadata": {},
   "outputs": [],
   "source": [
    "raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ea1d626-5a52-4013-89eb-6d1d519c149a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dir = os.path.join(data_dir, 'test')\n",
    "print(f\"\\033[94mNumber of notebooks present in test set  = \", len(list(glob.iglob(os.path.join(test_dir, '*.json')))))\n",
    "\n",
    "df_test = r.read_all_notebooks_(test_dir, 4, 2, desc=\"Tests NBs\")\n",
    "\n",
    "df_test = r.extract_features(df_test).reset_index()\n",
    "df_test['rank'] = 0\n",
    "\n",
    "MODEL_USELESS = ['id', 'cell_id', 'cell_type', ]\n",
    "\n",
    "CODE_TYPE = 'code'\n",
    "MKDN_TYPE = 'markdown'\n",
    "\n",
    "BERT_MODEL_NAME = 'distilbert-base-uncased'\n",
    "PROCESSORS_COUNT = psutil.cpu_count(logical=False)\n",
    "\n",
    "BATCH_SIZE = 8\n",
    "TOTAL_MAX_LEN = 256\n",
    "\n",
    "df_test.loc[df_test.cell_type == CODE_TYPE, 'rank'] = m.predict(\n",
    "    model_path=BERT_MODEL_NAME,\n",
    "    check_point=os.path.join(models_dir, 'code_bert_checkpoint.pt'), \n",
    "    batch_size=BATCH_SIZE, \n",
    "    num_workers=PROCESSORS_COUNT, \n",
    "    max_len=CD_MAX_LENGTH, \n",
    "    generated_columns_count=GENERATED_COLUMNS_COUNT,\n",
    "    total_max_len=TOTAL_MAX_LEN, \n",
    "    data=df_test[df_test.cell_type == CODE_TYPE], \n",
    "    drop=MODEL_USELESS\n",
    ")\n",
    "\n",
    "df_test.loc[df_test.cell_type == MKDN_TYPE, 'rank'] = m.predict(\n",
    "    model_path=BERT_MODEL_NAME,\n",
    "    check_point=os.path.join(models_dir, 'markdown_bert_checkpoint.pt'), \n",
    "    batch_size=BATCH_SIZE, \n",
    "    num_workers=PROCESSORS_COUNT, \n",
    "    max_len=MK_MAX_LENGTH, \n",
    "    generated_columns_count=GENERATED_COLUMNS_COUNT,\n",
    "    total_max_len=TOTAL_MAX_LEN, \n",
    "    data=df_test[df_test.cell_type == MKDN_TYPE], \n",
    "    drop=MODEL_USELESS\n",
    ")\n",
    "\n",
    "df_test = df_test.sort_values(\"rank\").groupby(\"id\")[\"cell_id\"].apply(lambda x: \" \".join(x)).reset_index()\n",
    "df_test.rename(columns={\"cell_id\": \"cell_order\"}, inplace=True)\n",
    "display(df_test.head())\n",
    "\n",
    "df_test.to_csv(\"submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e28b7c86-0c4f-4d1f-b9cb-fb02b0a4364d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8cb418e-c1f6-4869-9e3c-1ab06ca93679",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4d14459-ef30-489c-8018-7ad129fbcf34",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4f15cc8-a69c-4dab-b1ab-174ed13d5b0b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
